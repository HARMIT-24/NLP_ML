{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PSO-word2vec","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPoIGRbElxyB8B/cvJZ3S/A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tajVfWbtyI5P","executionInfo":{"status":"ok","timestamp":1620453086645,"user_tz":-330,"elapsed":6238,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"4c56d682-7531-4ccf-b2e9-dc3279fcaec9"},"source":["# Importing libraries\n","import os\n","import math\n","import random\n","import operator\n","import string\n","import logging\n","import gensim\n","from gensim import utils\n","from gensim.models.doc2vec import LabeledSentence\n","from gensim.models import Doc2Vec\n","from gensim.models import Word2Vec\n","import random\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","import string\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","stopwords = nltk.corpus.stopwords.words('english')\n","newStopWords = ['want','got','say','amp', 'thi', 'ain']\n","stopwords.extend(newStopWords)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"78EOBpHoyPKK","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1620453086646,"user_tz":-330,"elapsed":6229,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"5288cc4c-b461-47ac-80b8-17f774a17f7e"},"source":["# LOADING the dataset\n","df = pd.read_csv(\"https://raw.githubusercontent.com/MitHar24/dataset/main/New%20combined_Dataset.csv\")\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>@BDUTT Shame on your hate news n fake news.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>@AP Pita can suck my ass! What a shit show of ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@simonjames67 @Mr_John_Harvey_ @theJeremyVine ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>@GMB @SeanFletcherTV Oh wow.........what the h...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@KallipolisState @talkRADIO @JuliaHB1 @spikedo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Label                                              Tweet\n","0      1        @BDUTT Shame on your hate news n fake news.\n","1      1  @AP Pita can suck my ass! What a shit show of ...\n","2      0  @simonjames67 @Mr_John_Harvey_ @theJeremyVine ...\n","3      0  @GMB @SeanFletcherTV Oh wow.........what the h...\n","4      0  @KallipolisState @talkRADIO @JuliaHB1 @spikedo..."]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"gBRO-b1wyPHZ"},"source":["# Pre-processing the dataset\n","def preprocess(text_string):\n","  text_string = text_string.translate(string.punctuation)\n","    ## Convert words to lower case and split them\n","  text_string = text_string.lower().split()\n","    ## Remove stop words\n","  text_string = [w for w in text_string if not w in stopwords and len(w) >= 3]\n","  text_string = \" \".join(text_string)\n","  space_pattern = '\\s+'\n","  giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n","      '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n","  mention_regex = '@\\w+\\W+'\n","  excliamation_regex = '!+'\n","  RT_regex = 'RT'\n","  parsed_text = re.sub(space_pattern, ' ', text_string)\n","  parsed_text = re.sub(excliamation_regex,'! ',parsed_text)\n","  parsed_text = re.sub(giant_url_regex, '', parsed_text)\n","  parsed_text = re.sub(mention_regex, '', parsed_text)\n","  parsed_text = re.sub(RT_regex,'', parsed_text)\n","  parsed_text = re.sub('(.)\\\\1{2,}', '\\\\1', parsed_text)\n","  parsed_text = re.sub('bitche', 'bitch',parsed_text)\n","  parsed_text.replace(\"#\", \"\")\n","  parsed_text = parsed_text.split()\n","  stemmer = SnowballStemmer('english')\n","  stemmed_words = [stemmer.stem(word) for word in parsed_text]\n","  parsed_text = \" \".join(stemmed_words)\n","  parsed_text = re.sub('[^a-zA-Z]',' ', parsed_text)\n","  return parsed_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"Q4g-i0edyPFO","executionInfo":{"status":"ok","timestamp":1620453101820,"user_tz":-330,"elapsed":18769,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"4efeaa17-5b16-4ce3-bf90-d64bcf3f0b4e"},"source":["# Loading the processed / cleaned data\n","df['processed'] = df['Tweet'].apply(lambda x: preprocess(x))\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Tweet</th>\n","      <th>processed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>@BDUTT Shame on your hate news n fake news.</td>\n","      <td>shame hate news fake news</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>@AP Pita can suck my ass! What a shit show of ...</td>\n","      <td>pita suck ass  shit show organ put anim save</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@simonjames67 @Mr_John_Harvey_ @theJeremyVine ...</td>\n","      <td>mr john harvey  waitros i m littl worri bank b...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>@GMB @SeanFletcherTV Oh wow.........what the h...</td>\n","      <td>seanfletchertv wow what hell know  wolv relega...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@KallipolisState @talkRADIO @JuliaHB1 @spikedo...</td>\n","      <td>talkradio spikedonlin noth like outrag now  ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Label  ...                                          processed\n","0      1  ...                         shame hate news fake news \n","1      1  ...      pita suck ass  shit show organ put anim save \n","2      0  ...  mr john harvey  waitros i m littl worri bank b...\n","3      0  ...  seanfletchertv wow what hell know  wolv relega...\n","4      0  ...  talkradio spikedonlin noth like outrag now  ma...\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X7diD41tyPDb","executionInfo":{"status":"ok","timestamp":1620453106443,"user_tz":-330,"elapsed":14601,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"5a5fb30a-2605-40e6-d548-6819d5ba87e7"},"source":["# Loading Pre-trained Gensim model\n","!pip install wget"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting wget\n","  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=b125b41b6fb5943dee39bfdd40893ec918f443479dd1b25cbc0612e481de26a8\n","  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"tsE6bo7QyPAk","executionInfo":{"status":"ok","timestamp":1620453130580,"user_tz":-330,"elapsed":24124,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"d0b2d910-df5e-4a96-bff6-4b25887dec9d"},"source":["import wget\n","wget.download(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'GoogleNews-vectors-negative300.bin.gz'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"ZfzcwKdS1ZqA"},"source":["# Applying Word2Vec model\n","from gensim.models import Word2Vec\n","word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n","wv = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n","wv.init_sims(replace = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o0UHYCZJ1Zn3"},"source":["def word_averaging(wv, words):\n","    all_words, mean = set(), []\n","    \n","    for word in words:\n","        if isinstance(word, np.ndarray):\n","            mean.append(word)\n","        elif word in wv.vocab:\n","            mean.append(wv.syn0norm[wv.vocab[word].index])\n","            all_words.add(wv.vocab[word].index)\n","\n","    if not mean:\n","        logging.warning(\"cannot compute similarity with no input %s\", words)\n","        # FIXME: remove these examples in pre-processing\n","        return np.zeros(wv.vector_size,)\n","\n","    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n","    return mean\n","\n","def  word_averaging_list(wv, text_list):\n","    return np.vstack([word_averaging(wv, post) for post in text_list ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"csr5b96w1Zk5"},"source":["def w2v_tokenize_text(text):\n","    tokens = []\n","    for sent in nltk.sent_tokenize(text, language='english'):\n","        for word in nltk.word_tokenize(sent, language='english'):\n","            if len(word) < 2:\n","                continue\n","            tokens.append(word)\n","    return tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RlyYwCqT1ZiM"},"source":["# Splitting the training ans testing data\n","X = df['processed']\n","y = df['Label']\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 0)\n","\n","test_tokenized = X_test.apply(lambda x: w2v_tokenize_text(x)).values\n","train_tokenized = X_train.apply(lambda x: w2v_tokenize_text(x)).values\n","\n","X_train_word_average = word_averaging_list(wv,train_tokenized)\n","X_test_word_average = word_averaging_list(wv,test_tokenized)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e-Hu_WMOMZKC"},"source":["# Scaling the data\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train_word_average = sc.fit_transform(X_train_word_average)\n","X_test_word_average = sc.fit_transform(X_test_word_average)\n","\n","# from sklearn.preprocessing import MinMaxScaler\n","# sc = MinMaxScaler()\n","# X_train_word_average = sc.fit_transform(X_train_word_average)\n","# X_test_word_average = sc.fit_transform(X_test_word_average)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ksjTGg251hrH"},"source":["# change dt for different models \n","from sklearn.ensemble import RandomForestClassifier\n","def sigmoid(x):\n","  return (1/(1+np.exp(-1*x)))  \n","dt = RandomForestClassifier(n_estimators = 100)\n","def pso(X, Y, popu_size, maxItr=5):\n","  particles = []\n","  for i in range(popu_size):\n","      particles.append(np.random.choice([0, 1], size=300))\n","  particles = np.array(particles,dtype=float)\n","  velocities = np.random.rand(popu_size,300)\n","  itr = 0\n","  pbest = np.zeros((popu_size,300))\n","  pbest_score = np.ones((popu_size,),dtype=float)*0\n","  gbest_score = 0\n","  gbest = np.zeros((300,))\n","\n","\n","  while(itr < maxItr):\n","    # start = time()\n","    for i in range(popu_size):\n","      X_togive = np.multiply(X, particles[i])\n","      dt.fit(X_togive, Y)\n","      Xt = np.multiply(X_train_word_average, particles[i])\n","      score = dt.score(X_test_word_average,y_test)\n","      if score > pbest_score[i]:\n","        pbest_score[i] = score\n","        pbest[i] = particles[i]\n","      if score > gbest_score:\n","        gbest_score = score\n","        gbest = pbest[i]\n","    \n","    for i in range(popu_size):\n","      velocities[i] = 0.4*velocities[i] + 2*np.random.rand()*(pbest[i]-particles[i]) +2*np.random.rand()*(gbest-particles[i])\n","      \n","      for kk in range(300):\n","        if velocities[i][kk]>4.0:\n","          velocities[i][kk] =  4.0\n","        elif velocities[i][kk]<-4.0:\n","          velocities[i][kk] = -4.0\n","      velo_sig = sigmoid(velocities[i])\n","    \n","      for kk in range(97):\n","        particles[i][kk] = (np.random.randn()<velo_sig[kk])*1\n","    # end = time()\n","    print(\"Epochs:\",itr+1,\"Best Score:\",gbest_score)\n","    itr+=1\n","  return gbest"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yiwn8PfOVw-Z","executionInfo":{"status":"ok","timestamp":1620456328760,"user_tz":-330,"elapsed":1865413,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"6a7674aa-b9e2-4552-bc94-2c252893f24a"},"source":["new_x = pso(X_train_word_average, y_train,5,5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epochs: 1 Best Score: 0.7607076008367893\n","Epochs: 2 Best Score: 0.7623591588064741\n","Epochs: 3 Best Score: 0.7630564832825633\n","Epochs: 4 Best Score: 0.7630564832825633\n","Epochs: 5 Best Score: 0.7630564832825633\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"afrf8mGAVxZx"},"source":["X_togive = np.multiply(X_train_word_average,new_x)\n","dt.fit(X_togive, y_train)\n","y_pred = dt.predict(X_test_word_average)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJQC7a6C2JCj"},"source":["# Printing Metrics\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n","f1 = f1_score(y_pred, y_test)\n","ps = precision_score(y_pred, y_test)\n","rs = recall_score(y_pred, y_test)\n","asc = accuracy_score(y_pred, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Iwpdd-7OdR5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620456404783,"user_tz":-330,"elapsed":1938864,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"81cf4337-dc5f-44b2-cf41-58befbceff68"},"source":["print(f\"F1 Score {f1}\")\n","print(f\"Precesion Score {ps}\")\n","print(f\"Recall Score {rs}\")\n","print(f\"Accuracy Score {asc}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["F1 Score 0.7000046474880327\n","Precesion Score 0.6074856820198435\n","Recall Score 0.8257675438596491\n","Accuracy Score 0.7630931845707785\n"],"name":"stdout"}]}]}