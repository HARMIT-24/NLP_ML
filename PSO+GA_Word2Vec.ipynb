{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PSO+GA_Word2Vec.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZh0cX7WiwuI","executionInfo":{"status":"ok","timestamp":1620619341013,"user_tz":-330,"elapsed":2256,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"029119e7-94bc-483b-b0c2-c912de9e17d0"},"source":["# Importing libraries\n","import os\n","import math\n","import random\n","import operator\n","import string\n","import logging\n","import gensim\n","from gensim import utils\n","from gensim.models.doc2vec import LabeledSentence\n","from gensim.models import Doc2Vec\n","from gensim.models import Word2Vec\n","import random\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","import string\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","stopwords = nltk.corpus.stopwords.words('english')\n","newStopWords = ['want','got','say','amp', 'thi', 'ain']\n","stopwords.extend(newStopWords)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dmjDVos3iyzb"},"source":["# Cleaning the Dataset\n","def preprocess(text_string):\n","  text_string = text_string.translate(string.punctuation)\n","    ## Convert words to lower case and split them\n","  text_string = text_string.lower().split()\n","    ## Remove stop words\n","  text_string = [w for w in text_string if not w in stopwords and len(w) >= 3]\n","  text_string = \" \".join(text_string)\n","  space_pattern = '\\s+'\n","  giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n","      '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n","  mention_regex = '@\\w+\\W+'\n","  excliamation_regex = '!+'\n","  RT_regex = 'RT'\n","  parsed_text = re.sub(space_pattern, ' ', text_string)\n","  parsed_text = re.sub(excliamation_regex,'! ',parsed_text)\n","  parsed_text = re.sub(giant_url_regex, '', parsed_text)\n","  parsed_text = re.sub(mention_regex, '', parsed_text)\n","  parsed_text = re.sub(RT_regex,'', parsed_text)\n","  parsed_text = re.sub('(.)\\\\1{2,}', '\\\\1', parsed_text)\n","  parsed_text = re.sub('bitche', 'bitch',parsed_text)\n","  parsed_text.replace(\"#\", \"\")\n","  parsed_text = parsed_text.split()\n","  stemmer = SnowballStemmer('english')\n","  stemmed_words = [stemmer.stem(word) for word in parsed_text]\n","  parsed_text = \" \".join(stemmed_words)\n","  #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n","  parsed_text = re.sub('[^a-zA-Z]',' ', parsed_text)\n","  return parsed_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"eioz7TBfiy3B","executionInfo":{"status":"ok","timestamp":1620619341746,"user_tz":-330,"elapsed":2934,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"56e9fda8-58fa-4abd-a001-17a7f1dbd17c"},"source":["# Loading the dataset\n","df = pd.read_csv(\"https://raw.githubusercontent.com/MitHar24/dataset/main/New%20combined_Dataset.csv\")\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>@BDUTT Shame on your hate news n fake news.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>@AP Pita can suck my ass! What a shit show of ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@simonjames67 @Mr_John_Harvey_ @theJeremyVine ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>@GMB @SeanFletcherTV Oh wow.........what the h...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@KallipolisState @talkRADIO @JuliaHB1 @spikedo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Label                                              Tweet\n","0      1        @BDUTT Shame on your hate news n fake news.\n","1      1  @AP Pita can suck my ass! What a shit show of ...\n","2      0  @simonjames67 @Mr_John_Harvey_ @theJeremyVine ...\n","3      0  @GMB @SeanFletcherTV Oh wow.........what the h...\n","4      0  @KallipolisState @talkRADIO @JuliaHB1 @spikedo..."]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"qtUz-sQPizHr","executionInfo":{"status":"ok","timestamp":1620619357281,"user_tz":-330,"elapsed":18451,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"00c832a8-a588-4570-e8e9-d3e5f10bc635"},"source":["# Printing the Processed Dataset\n","df['processed'] = df['Tweet'].apply(lambda x: preprocess(x))\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Tweet</th>\n","      <th>processed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>@BDUTT Shame on your hate news n fake news.</td>\n","      <td>shame hate news fake news</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>@AP Pita can suck my ass! What a shit show of ...</td>\n","      <td>pita suck ass  shit show organ put anim save</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@simonjames67 @Mr_John_Harvey_ @theJeremyVine ...</td>\n","      <td>mr john harvey  waitros i m littl worri bank b...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>@GMB @SeanFletcherTV Oh wow.........what the h...</td>\n","      <td>seanfletchertv wow what hell know  wolv relega...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@KallipolisState @talkRADIO @JuliaHB1 @spikedo...</td>\n","      <td>talkradio spikedonlin noth like outrag now  ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Label  ...                                          processed\n","0      1  ...                         shame hate news fake news \n","1      1  ...      pita suck ass  shit show organ put anim save \n","2      0  ...  mr john harvey  waitros i m littl worri bank b...\n","3      0  ...  seanfletchertv wow what hell know  wolv relega...\n","4      0  ...  talkradio spikedonlin noth like outrag now  ma...\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcozD6bRizMu","executionInfo":{"status":"ok","timestamp":1620619359948,"user_tz":-330,"elapsed":21106,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"1d43b88c-093a-47fd-f019-65ae5a066fa9"},"source":["# Loading Pre-trained Gensim model\n","!pip install wget"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"8n15O4vpPPkh","executionInfo":{"status":"ok","timestamp":1620619384032,"user_tz":-330,"elapsed":45179,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"92c275aa-7c37-4a22-b190-860ed8eb1025"},"source":["import wget\n","wget.download(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'GoogleNews-vectors-negative300.bin (3).gz'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"ewyMhKVUPPgv"},"source":["# Applying Word2Vec model\n","from gensim.models import Word2Vec\n","word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n","wv = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n","wv.init_sims(replace = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OOFxCSm-PPe2"},"source":["def word_averaging(wv, words):\n","    all_words, mean = set(), []\n","    \n","    for word in words:\n","        if isinstance(word, np.ndarray):\n","            mean.append(word)\n","        elif word in wv.vocab:\n","            mean.append(wv.syn0norm[wv.vocab[word].index])\n","            all_words.add(wv.vocab[word].index)\n","\n","    if not mean:\n","        logging.warning(\"cannot compute similarity with no input %s\", words)\n","        # FIXME: remove these examples in pre-processing\n","        return np.zeros(wv.vector_size,)\n","\n","    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n","    return mean\n","\n","def  word_averaging_list(wv, text_list):\n","    return np.vstack([word_averaging(wv, post) for post in text_list ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_uodYA8PPbO"},"source":["def w2v_tokenize_text(text):\n","    tokens = []\n","    for sent in nltk.sent_tokenize(text, language='english'):\n","        for word in nltk.word_tokenize(sent, language='english'):\n","            if len(word) < 2:\n","                continue\n","            tokens.append(word)\n","    return tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6pZ-fWTPWAC"},"source":["# Splitting the training ans testing data\n","X = df['processed']\n","Y = df['Label']\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state = 0)\n","\n","test_tokenized = x_test.apply(lambda x: w2v_tokenize_text(x)).values\n","train_tokenized = x_train.apply(lambda x: w2v_tokenize_text(x)).values\n","\n","X_train = word_averaging_list(wv,train_tokenized)\n","X_test = word_averaging_list(wv,test_tokenized)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vzL-RfuzizRQ"},"source":["# Scaling the X parameters i.e. X_train and X_test\n","from sklearn.preprocessing import MinMaxScaler\n","sc_X = MinMaxScaler()\n","X_train = sc_X.fit_transform(X_train)\n","X_test = sc_X.fit_transform(X_test)\n","\n","# # Scaling the data\n","# from sklearn.preprocessing import MinMaxScaler\n","# sc = MinMaxScaler()\n","# X_train = sc.fit_transform(X_train)\n","# X_test = sc.fit_transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ncuOKMflizTj"},"source":["def reduce_features(solution, features):\n","    selected_elements_indices = numpy.where(solution == 1)[0]\n","    reduced_features = features[:, selected_elements_indices]\n","    return reduced_features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y13QshiAizVz"},"source":["def sigmoid(x):\n","  return (1/(1+np.exp(-1*x)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDAgyD6oizYL"},"source":["from time import time\n","def init(X, y, popu_size, size, model,val_x,particles_init=None):\n","  particles = np.random.randint(2, size=(popu_size,size))\n","  if particles_init is not None:\n","    particles = particles_init\n","\n","  velocities = np.random.rand(popu_size,size)\n","  pbest = np.zeros((popu_size,size))\n","  pbest_score = np.ones((popu_size,),dtype=float)*0\n","  model.fit(X,y)\n","  sc = model.score(val_x,y_test)\n","  gbest_score = sc\n","  gbest = np.ones((size,),dtype=float)\n","  gbest_popu = np.ones((popu_size,size))\n","  return particles,velocities,pbest,gbest,pbest_score,gbest_score,gbest_popu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R5wrKDCmizaj"},"source":["def check_fitness(X,Y,particles,pbest,gbest,pbest_score,gbest_score,popu_size,model,size,val_x,gbest_popu):\n","  best_itr = np.zeros((size,))\n","  best_itr_sc = 0\n","  for i in range(popu_size):\n","    # Error\n","    # print(particles[i].shape)\n","    X_togive = np.multiply(particles[i], X)\n","    test_d = np.multiply(particles[i], val_x)\n","    model.fit(X_togive, Y)\n","    score = model.score(test_d,y_test)\n","    \n","    if score > pbest_score[i]:\n","      pbest_score[i] = score\n","      pbest[i] = particles[i]\n","    \n","    if score > gbest_score:\n","      gbest_score = score\n","      gbest = pbest[i]\n","      gbest_popu = pbest\n","    \n","    if score >best_itr_sc:\n","      best_itr_sc = score\n","      best_itr = particles[i]\n","  return pbest,gbest,pbest_score,gbest_score,best_itr_sc,best_itr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHux8bTKizc5"},"source":["def update_particle(velocities,particles,pbest,gbest,popu_size,size,itr,maxItr,best_itr):\n","  wmax = 1.0\n","  wmin = 0.6\n","  kmin = 1.5\n","  kmax = 4.0\n","  w = wmax-(itr/maxItr)*(wmax-wmin)\n","  k = kmin+(itr/maxItr)*(wmax-wmin)\n","  for i in range(popu_size):\n","    velocities[i] = w*velocities[i] + k*np.random.rand()*(pbest[i]-particles[i]) + k*np.random.rand()*(gbest-particles[i])+ k*np.random.rand()*(best_itr-particles[i])\n","    for kk in range(size):\n","      if velocities[i][kk]>4.0:\n","        velocities[i][kk] = 4.0\n","      elif velocities[i][kk]<-4.0:\n","        velocities[i][kk] = -4.0\n","    velo_sig = sigmoid(velocities[i])\n","    for kk in range(size):\n","        particles[i][kk] = (np.random.randn()<velo_sig[kk])*1\n","  return particles"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKnWdMQxizfD"},"source":["def pso(X, Y, popu_size, model, maxItr, size,val_x,particles_init=None):\n","  all_scores = []\n","  particles,velocities,pbest,gbest,pbest_score,gbest_score,gbest_popu = init(X,Y,popu_size,size,model,val_x,particles_init)\n","  itr = 0\n","  early_stop = 0\n","  while(itr < maxItr):\n","    start = time()\n","    pbest,gbest,pbest_score,gbest_score,best_itr_sc,best_itr = check_fitness(X,Y,particles,pbest,gbest,pbest_score,gbest_score,popu_size,model,size,val_x,gbest_popu)\n","    particles = update_particle(velocities,particles,pbest,gbest,popu_size,size,itr,maxItr,best_itr)\n","    end = time()\n","    if all_scores:\n","      if gbest_score==all_scores[-1]:\n","        early_stop+=1\n","        if early_stop==35:\n","          print(\"Early Stopping due to no increase in fitness...\")\n","          return gbest,gbest_popu,all_scores\n","      else:\n","        early_stop = 0\n","    print(\"Epochs:\",itr+1,\"Time Taken:\",round((end - start), 2),\"secs\",\"Best Score:\",gbest_score,\"Best Score Itr\",best_itr_sc)\n","    all_scores.append(gbest_score)\n","    itr+=1\n","  print(\"Best Score \",gbest_score)\n","  print(\"Best Features: \",gbest)\n","  gbest_indices = np.where(gbest == 1)[0]\n","  gbest_num_elements = gbest_indices.shape[0]\n","  print(\"Indices Selected: \",gbest_indices)\n","  print(\"Total Elements Selected: \",gbest_num_elements)\n","  return gbest,gbest_popu,all_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6PMciMt2izhX","executionInfo":{"status":"ok","timestamp":1620623395716,"user_tz":-330,"elapsed":4056765,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"55f7f3be-8753-4851-80bb-d0bf5dd0f142"},"source":["from sklearn.ensemble import RandomForestClassifier\n","lr = RandomForestClassifier(n_estimators = 100)\n","new_x,best_popu,all_scores = pso(X_train,y_train,8,lr,5,300,X_test,None)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epochs: 1 Time Taken: 552.39 secs Best Score: 0.7594230557492568 Best Score Itr 0.7586890299849525\n","Epochs: 2 Time Taken: 787.92 secs Best Score: 0.7595698609021176 Best Score Itr 0.7595698609021176\n","Epochs: 3 Time Taken: 795.82 secs Best Score: 0.7615884317539545 Best Score Itr 0.7615884317539545\n","Epochs: 4 Time Taken: 754.39 secs Best Score: 0.7615884317539545 Best Score Itr 0.7607443021250045\n","Epochs: 5 Time Taken: 801.44 secs Best Score: 0.7630197819943481 Best Score Itr 0.7630197819943481\n","Best Score  0.7630197819943481\n","Best Features:  [1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n"," 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0.\n"," 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0.\n"," 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n"," 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.\n"," 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0.\n"," 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n"," 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.\n"," 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.]\n","Indices Selected:  [  0   1   2   3   5   6   8   9  10  11  12  14  15  16  17  18  19  20\n","  21  23  24  25  27  28  29  32  34  35  36  39  40  42  43  45  49  50\n","  51  52  53  57  58  59  60  63  64  65  66  67  68  69  70  71  73  75\n","  77  78  80  81  82  83  84  85  86  87  88  89  90  93  94  96  98  99\n"," 100 101 102 103 104 106 107 109 110 111 115 116 117 120 123 124 125 126\n"," 127 129 130 135 138 139 140 141 142 143 144 145 146 147 148 149 150 151\n"," 154 155 156 157 158 159 160 161 163 165 166 167 168 169 171 172 174 175\n"," 176 178 179 180 181 182 183 184 186 187 188 190 193 194 195 196 197 198\n"," 199 200 206 207 208 209 211 212 213 214 217 218 219 220 223 224 226 229\n"," 230 231 233 234 236 237 238 239 240 241 242 243 244 248 249 251 252 254\n"," 255 256 257 259 261 262 263 266 267 268 269 270 271 272 273 274 276 277\n"," 278 280 281 283 284 285 286 287 288 289 291 292 294 295 296 297]\n","Total Elements Selected:  214\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kDFM9pFfizjk"},"source":["x_to_train = np.multiply(new_x,X_train)\n","xTest = np.multiply(new_x,X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"atDTiThdizlS"},"source":["def reduce_features(solution, features):\n","    selected_elements_indices = numpy.where(solution == 1)[0]\n","    reduced_features = features[:, selected_elements_indices]\n","    return reduced_features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SJ7ojOLIiznt"},"source":["def cal_pop_fitness(pop, features, labels):#, train_indices, test_indices):\n","    accuracies = numpy.zeros(pop.shape[0])\n","    idx = 0\n","    for curr_solution in pop:\n","        reduced_features = reduce_features(curr_solution, x_to_train)\n","        reduced_features_t = reduce_features(curr_solution, xTest)\n","        #train_data = reduced_features[train_indices, :]\n","        #test_data = reduced_features[test_indices, :]\n","\n","        #train_labels = labels[train_indices]\n","        #test_labels = labels[test_indices]\n","        # train_data,test_data,train_labels,test_labels = train_test_split(reduced_features,labels, test_size=0.05, random_state=42)\n","        #SV_classifier = SVC(kernel='rbf')\n","        SV_classifier = RandomForestClassifier(n_estimators = 100)\n","        SV_classifier.fit(X=reduced_features, y=y_train)\n","        #predictions = SV_classifier.predict(test_data)\n","        accuracies[idx] = SV_classifier.score(reduced_features_t,y_test)\n","        idx = idx + 1\n","    return accuracies"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBUxTnSzjObE"},"source":["def select_mating_pool(pop, fitness, num_parents):\n","    # Selecting the best individuals in the current generation as parents for producing the offspring of the next generation.\n","    parents = numpy.empty((num_parents, pop.shape[1]))\n","    for parent_num in range(num_parents):\n","        max_fitness_idx = numpy.where(fitness == numpy.max(fitness))\n","        max_fitness_idx = max_fitness_idx[0][0]\n","        parents[parent_num, :] = pop[max_fitness_idx, :]\n","        fitness[max_fitness_idx] = -99999999999\n","    return parents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C0DXVnCIjOdU"},"source":["def crossover(parents, offspring_size):\n","    offspring = numpy.empty(offspring_size)\n","    # The point at which crossover takes place between two parents. Usually, it is at the center.\n","    crossover_point = numpy.uint8(offspring_size[1]/2)\n","    for k in range(offspring_size[0]):\n","        # Index of the first parent to mate.\n","        parent1_idx = k%parents.shape[0]\n","        # Index of the second parent to mate.\n","        parent2_idx = (k+1)%parents.shape[0]\n","        # The new offspring will have its first half of its genes taken from the first parent.\n","        offspring[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]\n","        # The new offspring will have its second half of its genes taken from the second parent.\n","        offspring[k, crossover_point:] = parents[parent2_idx, crossover_point:]\n","    return offspring\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"twVCUGjrjOfj"},"source":["def mutation(offspring_crossover, num_mutations=2):\n","    mutation_idx = numpy.random.randint(low=0, high=offspring_crossover.shape[1], size=num_mutations)\n","    # Mutation changes a single gene in each offspring randomly.\n","    for idx in range(offspring_crossover.shape[0]):\n","        # The random value to be added to the gene.\n","        offspring_crossover[idx, mutation_idx] = 1 - offspring_crossover[idx, mutation_idx]\n","    return offspring_crossover"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"So15WReHjOiz"},"source":["import numpy\n","import pickle\n","import matplotlib.pyplot\n","data_inputs = x_to_train\n","data_outputs = xTest\n","num_samples = data_inputs.shape[0]\n","num_feature_elements = data_inputs.shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zBuZfWCjOlQ","executionInfo":{"status":"ok","timestamp":1620623395759,"user_tz":-330,"elapsed":4056742,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"0adb8184-f2eb-4370-d1b5-3ffd4c62867f"},"source":["sol_per_pop = 8 # Population size.\n","num_parents_mating = 4 # Number of parents inside the mating pool.\n","num_mutations = 3 # Number of elements to mutate.\n","# Defining the population shape.\n","pop_shape = (sol_per_pop, num_feature_elements)\n","# Creating the initial population.\n","new_population = best_popu\n","print(new_population.shape)\n","best_outputs = []"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(8, 300)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XWSymXmljOnF","executionInfo":{"status":"ok","timestamp":1620628136833,"user_tz":-330,"elapsed":8797801,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"05b2ebd0-9f2b-42d1-ff21-c7fabad7dbb5"},"source":["num_generations = 5\n","for generation in range(num_generations):\n","    #print(\"Generation : \", generation)\n","    start = time()\n","    # Measuring the fitness of each chromosome in the population.\n","    fitness = cal_pop_fitness(new_population, data_inputs, data_outputs)#, train_indices, test_indices)\n","    best_outputs.append(numpy.max(fitness))\n","    # The best result in the current iteration.\n","    #print(\"Best result : \", best_outputs[-1])\n","    # Selecting the best parents in the population for mating.\n","    parents = select_mating_pool(new_population, fitness, num_parents_mating)\n","    # Generating next generation using crossover.\n","    offspring_crossover = crossover(parents, offspring_size=(pop_shape[0]-parents.shape[0], num_feature_elements))\n","    # Adding some variations to the offspring using mutation.\n","    offspring_mutation = mutation(offspring_crossover, num_mutations=num_mutations)\n","    # Creating the new population based on the parents and offspring.\n","    new_population[0:parents.shape[0], :] = parents\n","    new_population[parents.shape[0]:, :] = offspring_mutation\n","    end = time()\n","    best_match_idx = numpy.where(fitness == numpy.max(fitness))[0]\n","    best_match_idx = best_match_idx[0]\n","    best_solution = new_population[best_match_idx, :]\n","    print(\"Epochs:\",generation,\"Time Taken:\",(end-start),\"secs\",\"Best result : \", best_outputs[-1])\n","# Getting the best solution after iterating finishing all generations.\n","# At first, the fitness is calculated for each solution in the final generation.\n","fitness = cal_pop_fitness(new_population, data_inputs, data_outputs)#,train_indices, test_indices)\n","# Then return the index of that solution corresponding to the best fitness.\n","best_match_idx = numpy.where(fitness == numpy.max(fitness))[0]\n","best_match_idx = best_match_idx[0]\n","best_solution = new_population[best_match_idx, :]\n","best_solution_indices = numpy.where(best_solution == 1)[0]\n","best_solution_num_elements = best_solution_indices.shape[0]\n","best_solution_fitness = fitness[best_match_idx]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epochs: 0 Time Taken: 799.9246525764465 secs Best result :  0.7627628729768415\n","Epochs: 1 Time Taken: 801.8285043239594 secs Best result :  0.7621389510771828\n","Epochs: 2 Time Taken: 790.3625564575195 secs Best result :  0.761331522736448\n","Epochs: 3 Time Taken: 788.9676730632782 secs Best result :  0.7615517304657393\n","Epochs: 4 Time Taken: 798.1072866916656 secs Best result :  0.7622857562300437\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gWo55FPtjZU2"},"source":["fitness = cal_pop_fitness(new_population, data_inputs, data_outputs)#,train_indices, test_indices)\n","# Then return the index of that solution corresponding to the best fitness.\n","best_match_idx = numpy.where(fitness == numpy.max(fitness))[0]\n","best_match_idx = best_match_idx[0]\n","best_solution = new_population[best_match_idx, :]\n","best_solution_indices = numpy.where(best_solution == 1)[0]\n","best_solution_num_elements = best_solution_indices.shape[0]\n","best_solution_fitness = fitness[best_match_idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h7fLDoTbjZS-","executionInfo":{"status":"ok","timestamp":1620628906757,"user_tz":-330,"elapsed":30,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"735c322d-97dc-4371-d8d7-074482a473d2"},"source":["print(\"best_match_idx : \", best_match_idx)\n","print(\"best_solution : \", best_solution)\n","print(\"Selected indices : \", best_solution_indices)\n","print(\"Number of selected elements : \", best_solution_num_elements)\n","print(\"Best solution fitness : \", best_solution_fitness)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["best_match_idx :  2\n","best_solution :  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n"," 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n","Selected indices :  [  0   1   2   3   4   5   6   7   8   9  10  12  13  14  15  16  17  18\n","  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n","  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n","  55  56  57  58  59  60  61  62  63  64  65  67  68  69  70  71  72  73\n","  74  75  76  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92\n","  93  94  95  96  97  98  99 100 101 102 103 104 106 107 108 109 110 111\n"," 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129\n"," 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147\n"," 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165\n"," 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 183 184\n"," 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202\n"," 203 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221\n"," 222 223 224 225 227 228 229 230 231 232 233 234 235 236 237 238 239 240\n"," 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258\n"," 259 261 263 264 265 266 267 269 270 271 272 273 274 275 276 277 278 279\n"," 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 298\n"," 299]\n","Number of selected elements :  289\n","Best solution fitness :  0.7607443021250045\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"upe3AsrljZRQ"},"source":["scores = all_scores + best_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7RNrov0wZGRC"},"source":["X_temp = X.apply(lambda x: w2v_tokenize_text(x)).values\n","X = word_averaging_list(wv,X_temp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OGtDY8lajZPE"},"source":["best_solution\n","X_new = np.multiply(X,best_solution)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LfvI486AjZMs","executionInfo":{"status":"ok","timestamp":1620629766173,"user_tz":-330,"elapsed":125029,"user":{"displayName":"Harsh Mittal","photoUrl":"","userId":"12434610001146997162"}},"outputId":"3b0f8241-5d67-4dc7-ad7d-b7333a2718f6"},"source":["from sklearn.model_selection import train_test_split\n","train_data,test_data,train_labels,test_labels = train_test_split(X_new,Y, test_size = 0.33, random_state = 0)\n","\n","\n","\n","# # Scaling the data\n","# from sklearn.preprocessing import MinMaxScaler\n","# sc = MinMaxScaler()\n","# train_data = sc.fit_transform(train_data)\n","# test_data = sc.fit_transform(test_data)\n","\n","# Scaling the data\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","train_data = sc.fit_transform(train_data)\n","test_data = sc.fit_transform(test_data)\n","\n","\n","classifier = RandomForestClassifier(n_estimators = 100)\n","classifier.fit(train_data,train_labels)\n","Y_pred = classifier.predict(test_data)\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n","Y_test = test_labels\n","f1 = f1_score(Y_test, Y_pred)\n","ps = precision_score(Y_test, Y_pred)\n","rs = recall_score(Y_test, Y_pred)\n","asc = accuracy_score(Y_test, Y_pred)\n","print(f\"F1 Score {f1}\")\n","print(f\"Precesion Score {ps}\")\n","print(f\"Recall Score {rs}\")\n","print(f\"Accuracy Score {asc}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["F1 Score 0.7049769112365316\n","Precesion Score 0.8357664233576643\n","Recall Score 0.6095829636202307\n","Accuracy Score 0.7678643520387566\n"],"name":"stdout"}]}]}