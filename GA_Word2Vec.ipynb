{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GA_Word2Vec.ipynb","provenance":[{"file_id":"1e4kGeqnF4LobqwX96PqR2PfFdShF5wtR","timestamp":1620626972680}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GH8Be4q6dy5C","executionInfo":{"status":"ok","timestamp":1620651930377,"user_tz":-330,"elapsed":5274,"user":{"displayName":"2K19/EP/036 HARSH MITTAL","photoUrl":"","userId":"14192984764423656819"}},"outputId":"6aa634c2-7ed3-442b-84d6-5f2b75e8975d"},"source":["# Importing libraries\n","import os\n","import math\n","import random\n","import operator\n","import string\n","import logging\n","import gensim\n","from gensim import utils\n","from gensim.models.doc2vec import LabeledSentence\n","from gensim.models import Doc2Vec\n","from gensim.models import Word2Vec\n","import random\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","import string\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","stopwords = nltk.corpus.stopwords.words('english')\n","newStopWords = ['want','got','say','amp', 'thi', 'ain']\n","stopwords.extend(newStopWords)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"dvydDHVjd4Rq","executionInfo":{"status":"ok","timestamp":1620651931864,"user_tz":-330,"elapsed":6739,"user":{"displayName":"2K19/EP/036 HARSH MITTAL","photoUrl":"","userId":"14192984764423656819"}},"outputId":"f45f46d3-db1f-42b4-9d2a-db84865c295a"},"source":["# Loading the dataset\n","df = pd.read_csv(\"https://raw.githubusercontent.com/MitHar24/dataset/main/New%20combined_Dataset.csv\")\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>@BDUTT Shame on your hate news n fake news.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>@AP Pita can suck my ass! What a shit show of ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@simonjames67 @Mr_John_Harvey_ @theJeremyVine ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>@GMB @SeanFletcherTV Oh wow.........what the h...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@KallipolisState @talkRADIO @JuliaHB1 @spikedo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Label                                              Tweet\n","0      1        @BDUTT Shame on your hate news n fake news.\n","1      1  @AP Pita can suck my ass! What a shit show of ...\n","2      0  @simonjames67 @Mr_John_Harvey_ @theJeremyVine ...\n","3      0  @GMB @SeanFletcherTV Oh wow.........what the h...\n","4      0  @KallipolisState @talkRADIO @JuliaHB1 @spikedo..."]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"TsEidnQUd4Nb"},"source":["# Cleaning the Dataset\n","def preprocess(text_string):\n","  text_string = text_string.translate(string.punctuation)\n","    ## Convert words to lower case and split them\n","  text_string = text_string.lower().split()\n","    ## Remove stop words\n","  text_string = [w for w in text_string if not w in stopwords and len(w) >= 3]\n","  text_string = \" \".join(text_string)\n","  space_pattern = '\\s+'\n","  giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n","      '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n","  mention_regex = '@\\w+\\W+'\n","  excliamation_regex = '!+'\n","  RT_regex = 'RT'\n","  parsed_text = re.sub(space_pattern, ' ', text_string)\n","  parsed_text = re.sub(excliamation_regex,'! ',parsed_text)\n","  parsed_text = re.sub(giant_url_regex, '', parsed_text)\n","  parsed_text = re.sub(mention_regex, '', parsed_text)\n","  parsed_text = re.sub(RT_regex,'', parsed_text)\n","  parsed_text = re.sub('(.)\\\\1{2,}', '\\\\1', parsed_text)\n","  parsed_text = re.sub('bitche', 'bitch',parsed_text)\n","  parsed_text.replace(\"#\", \"\")\n","  parsed_text = parsed_text.split()\n","  stemmer = SnowballStemmer('english')\n","  stemmed_words = [stemmer.stem(word) for word in parsed_text]\n","  parsed_text = \" \".join(stemmed_words)\n","  #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n","  parsed_text = re.sub('[^a-zA-Z]',' ', parsed_text)\n","  return parsed_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"xYT86q5Ud4Kg","executionInfo":{"status":"ok","timestamp":1620651946232,"user_tz":-330,"elapsed":21077,"user":{"displayName":"2K19/EP/036 HARSH MITTAL","photoUrl":"","userId":"14192984764423656819"}},"outputId":"15ed782c-c8b9-4bc1-a214-f65730402ea0"},"source":["# Printing the Processed Dataset\n","df['processed'] = df['Tweet'].apply(lambda x: preprocess(x))\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Label</th>\n","      <th>Tweet</th>\n","      <th>processed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>@BDUTT Shame on your hate news n fake news.</td>\n","      <td>shame hate news fake news</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>@AP Pita can suck my ass! What a shit show of ...</td>\n","      <td>pita suck ass  shit show organ put anim save</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@simonjames67 @Mr_John_Harvey_ @theJeremyVine ...</td>\n","      <td>mr john harvey  waitros i m littl worri bank b...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>@GMB @SeanFletcherTV Oh wow.........what the h...</td>\n","      <td>seanfletchertv wow what hell know  wolv relega...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@KallipolisState @talkRADIO @JuliaHB1 @spikedo...</td>\n","      <td>talkradio spikedonlin noth like outrag now  ma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Label  ...                                          processed\n","0      1  ...                         shame hate news fake news \n","1      1  ...      pita suck ass  shit show organ put anim save \n","2      0  ...  mr john harvey  waitros i m littl worri bank b...\n","3      0  ...  seanfletchertv wow what hell know  wolv relega...\n","4      0  ...  talkradio spikedonlin noth like outrag now  ma...\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"rdwzMZn6d4GY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620651952298,"user_tz":-330,"elapsed":27118,"user":{"displayName":"2K19/EP/036 HARSH MITTAL","photoUrl":"","userId":"14192984764423656819"}},"outputId":"db2aedb0-e3bf-4c2d-dba3-ac16e7c59775"},"source":["# Loading Pre-trained Gensim model\n","!pip install wget"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting wget\n","  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=1aa49e246a4e8128a18e037e18ffaf6556a0989238c9db58648c7aa132b0baa9\n","  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"GMdJVmBQ98G-","executionInfo":{"status":"ok","timestamp":1620651974526,"user_tz":-330,"elapsed":49313,"user":{"displayName":"2K19/EP/036 HARSH MITTAL","photoUrl":"","userId":"14192984764423656819"}},"outputId":"4c969bc9-e9d9-49e1-8272-13ab16dfccf8"},"source":["import wget\n","wget.download(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'GoogleNews-vectors-negative300.bin.gz'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"SRmk82hd98Cg"},"source":["# Applying Word2Vec model\n","from gensim.models import Word2Vec\n","word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n","wv = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n","wv.init_sims(replace = True)\n","\n","\n","def word_averaging(wv, words):\n","    all_words, mean = set(), []\n","    \n","    for word in words:\n","        if isinstance(word, np.ndarray):\n","            mean.append(word)\n","        elif word in wv.vocab:\n","            mean.append(wv.syn0norm[wv.vocab[word].index])\n","            all_words.add(wv.vocab[word].index)\n","\n","    if not mean:\n","        logging.warning(\"cannot compute similarity with no input %s\", words)\n","        # FIXME: remove these examples in pre-processing\n","        return np.zeros(wv.vector_size,)\n","\n","    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n","    return mean\n","\n","def  word_averaging_list(wv, text_list):\n","    return np.vstack([word_averaging(wv, post) for post in text_list ])\n","\n","\n","def w2v_tokenize_text(text):\n","    tokens = []\n","    for sent in nltk.sent_tokenize(text, language='english'):\n","        for word in nltk.word_tokenize(sent, language='english'):\n","            if len(word) < 2:\n","                continue\n","            tokens.append(word)\n","    return tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qsb4Z5xa979t"},"source":["# Splitting the training ans testing data\n","X = df['processed']\n","y = df['Label']\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 0)\n","\n","test_tokenized = X_test.apply(lambda x: w2v_tokenize_text(x)).values\n","train_tokenized = X_train.apply(lambda x: w2v_tokenize_text(x)).values\n","\n","X_train_word_average = word_averaging_list(wv,train_tokenized)\n","X_test_word_average = word_averaging_list(wv,test_tokenized)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Glf5H-sud4CJ"},"source":["# Scaling the data\n","from sklearn.preprocessing import MinMaxScaler\n","sc = MinMaxScaler()\n","X_train_word_average = sc.fit_transform(X_train_word_average)\n","X_test_word_average = sc.fit_transform(X_test_word_average)\n","\n","# from sklearn.preprocessing import MinMaxScaler\n","# sc = MinMaxScaler()\n","# X_train_word_average = sc.fit_transform(X_train_word_average)\n","# X_test_word_average = sc.fit_transform(X_test_word_average)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nnSZgMatd385"},"source":["X_train = X_train_word_average\n","X_test = X_test_word_average"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x9o3PBzod3RJ"},"source":["import numpy\n","from sklearn.ensemble import RandomForestClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"00z72ecmiXxF"},"source":["import numpy\n","def reduce_features(solution, features):\n","    selected_elements_indices = numpy.where(solution == 1)[0]\n","    reduced_features = features[:, selected_elements_indices]\n","    return reduced_features\n","\n","\n","def classification_accuracy(labels, predictions):\n","    correct = numpy.where(labels == predictions)[0]\n","    accuracy = correct.shape[0]/labels.shape[0]\n","    return accuracy\n","\n","\n","def cal_pop_fitness(pop, features, labels, train_indices, test_indices):\n","    accuracies = numpy.zeros(pop.shape[0])\n","    idx = 0\n","\n","    for curr_solution in pop:\n","        reduced_features = reduce_features(curr_solution, features)\n","        #train_data = reduced_features[train_indices, :]\n","        #test_data = reduced_features[test_indices, :]\n","\n","        #train_labels = labels[train_indices]\n","        #test_labels = labels[test_indices]\n","        train_data,test_data,train_labels,test_labels = train_test_split(reduced_features,labels, test_size=0.05, random_state=42)\n","        #SV_classifier = SVC(kernel='rbf')\n","        SV_classifier = RandomForestClassifier(n_estimators = 100)\n","        SV_classifier.fit(X=train_data, y=train_labels)\n","\n","        #predictions = SV_classifier.predict(test_data)\n","        accuracies[idx] = SV_classifier.score(test_data,test_labels)\n","        idx = idx + 1\n","    return accuracies\n","\n","def select_mating_pool(pop, fitness, num_parents):\n","    # Selecting the best individuals in the current generation as parents for producing the offspring of the next generation.\n","    parents = numpy.empty((num_parents, pop.shape[1]))\n","    for parent_num in range(num_parents):\n","        max_fitness_idx = numpy.where(fitness == numpy.max(fitness))\n","        max_fitness_idx = max_fitness_idx[0][0]\n","        parents[parent_num, :] = pop[max_fitness_idx, :]\n","        fitness[max_fitness_idx] = -99999999999\n","    return parents\n","\n","\n","def crossover(parents, offspring_size):\n","    offspring = numpy.empty(offspring_size)\n","    # The point at which crossover takes place between two parents. Usually, it is at the center.\n","    crossover_point = numpy.uint8(offspring_size[1]/2)\n","\n","    for k in range(offspring_size[0]):\n","        # Index of the first parent to mate.\n","        parent1_idx = k%parents.shape[0]\n","        # Index of the second parent to mate.\n","        parent2_idx = (k+1)%parents.shape[0]\n","        # The new offspring will have its first half of its genes taken from the first parent.\n","        offspring[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]\n","        # The new offspring will have its second half of its genes taken from the second parent.\n","        offspring[k, crossover_point:] = parents[parent2_idx, crossover_point:]\n","    return offspring\n","\n","\n","def mutation(offspring_crossover, num_mutations=2):\n","    mutation_idx = numpy.random.randint(low=0, high=offspring_crossover.shape[1], size=num_mutations)\n","    # Mutation changes a single gene in each offspring randomly.\n","    for idx in range(offspring_crossover.shape[0]):\n","        # The random value to be added to the gene.\n","        offspring_crossover[idx, mutation_idx] = 1 - offspring_crossover[idx, mutation_idx]\n","    return offspring_crossover"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HbMVuPF3iXj1","executionInfo":{"status":"ok","timestamp":1620656319095,"user_tz":-330,"elapsed":4393801,"user":{"displayName":"2K19/EP/036 HARSH MITTAL","photoUrl":"","userId":"14192984764423656819"}},"outputId":"3d8bb2a7-b91b-41d2-85fe-21a1deb72b90"},"source":["import numpy\n","import pickle\n","import matplotlib.pyplot\n","from time import time\n","\n","data_inputs = X_train\n","data_outputs = y_train\n","\n","num_samples = data_inputs.shape[0]\n","num_feature_elements = data_inputs.shape[1]\n","# train_indices = y_train\n","# test_indices = y_test\n","train_indices = numpy.arange(1, num_samples,2)\n","test_indices = numpy.arange(0, num_samples,2)\n","\"\"\"\n","Genetic algorithm parameters:\n","    Population size\n","    Mating pool size\n","    Number of mutations\n","\"\"\"\n","sol_per_pop = 8 # Population size.\n","num_parents_mating = 4 # Number of parents inside the mating pool.\n","num_mutations = 3 # Number of elements to mutate.\n","\n","# Defining the population shape.\n","pop_shape = (sol_per_pop, num_feature_elements)\n","\n","# Creating the initial population.\n","new_population = numpy.random.randint(low=0, high=2, size=pop_shape)\n","print(new_population.shape)\n","\n","best_outputs = []\n","num_generations = 6\n","for generation in range(num_generations):\n","    #print(\"Generation : \", generation)\n","    start = time()\n","    # Measuring the fitness of each chromosome in the population.\n","    fitness = cal_pop_fitness(new_population, data_inputs, data_outputs, train_indices, test_indices)\n","\n","    best_outputs.append(numpy.max(fitness))\n","    # The best result in the current iteration.\n","    #print(\"Best result : \", best_outputs[-1])\n","\n","    # Selecting the best parents in the population for mating.\n","    parents = select_mating_pool(new_population, fitness, num_parents_mating)\n","\n","    # Generating next generation using crossover.\n","    offspring_crossover = crossover(parents, offspring_size=(pop_shape[0]-parents.shape[0], num_feature_elements))\n","\n","    # Adding some variations to the offspring using mutation.\n","    offspring_mutation = mutation(offspring_crossover, num_mutations=num_mutations)\n","\n","    # Creating the new population based on the parents and offspring.\n","    new_population[0:parents.shape[0], :] = parents\n","    new_population[parents.shape[0]:, :] = offspring_mutation\n","    end = time()\n","    best_match_idx = numpy.where(fitness == numpy.max(fitness))[0]\n","    best_match_idx = best_match_idx[0]\n","    best_solution = new_population[best_match_idx, :]\n","    print(\"Epochs:\",generation,\"Time Taken:\",(end-start),\"secs\",\"Best result : \", best_outputs[-1])\n","# Getting the best solution after iterating finishing all generations.\n","# At first, the fitness is calculated for each solution in the final generation.\n","fitness = cal_pop_fitness(new_population, data_inputs, data_outputs, train_indices, test_indices)\n","# Then return the index of that solution corresponding to the best fitness.\n","best_match_idx = numpy.where(fitness == numpy.max(fitness))[0]\n","best_match_idx = best_match_idx[0]\n","\n","best_solution = new_population[best_match_idx, :]\n","best_solution_indices = numpy.where(best_solution == 1)[0]\n","best_solution_num_elements = best_solution_indices.shape[0]\n","best_solution_fitness = fitness[best_match_idx]\n","\n","print(\"best_match_idx : \", best_match_idx)\n","print(\"best_solution : \", best_solution)\n","print(\"Selected indices : \", best_solution_indices)\n","print(\"Number of selected elements : \", best_solution_num_elements)\n","print(\"Best solution fitness : \", best_solution_fitness)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(8, 300)\n","Epochs: 0 Time Taken: 592.0842967033386 secs Best result :  0.7639190166305134\n","Epochs: 1 Time Taken: 580.8229393959045 secs Best result :  0.759942154736081\n","Epochs: 2 Time Taken: 589.382297039032 secs Best result :  0.765003615328995\n","Epochs: 3 Time Taken: 594.862701177597 secs Best result :  0.7668112798264642\n","Epochs: 4 Time Taken: 607.8037297725677 secs Best result :  0.7628344179320318\n","Epochs: 5 Time Taken: 606.5711450576782 secs Best result :  0.767172812725958\n","best_match_idx :  5\n","best_solution :  [0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1\n"," 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0\n"," 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1\n"," 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0\n"," 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0\n"," 0 1 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1\n"," 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1\n"," 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1\n"," 1 0 0 0]\n","Selected indices :  [  4   5   6   7   8   9  11  17  19  24  27  28  29  30  32  34  36  37\n","  39  40  43  45  47  51  56  58  62  63  67  68  69  70  72  75  76  77\n","  83  90  93  94  95  97  98  99 103 104 106 107 108 110 111 114 116 121\n"," 122 123 124 126 128 129 134 135 137 139 140 141 143 146 148 149 155 158\n"," 161 163 164 166 168 169 170 171 172 175 182 186 187 188 191 193 194 195\n"," 199 202 204 205 209 213 214 216 219 221 222 223 224 225 226 227 228 230\n"," 231 232 235 236 237 238 243 246 248 249 251 252 253 258 260 262 265 271\n"," 274 276 277 279 281 283 284 286 287 288 289 290 293 295 296]\n","Number of selected elements :  141\n","Best solution fitness :  0.764642082429501\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tbEMEiMkBc3x"},"source":["X_temp =  X.apply(lambda x: w2v_tokenize_text(x)).values\n","X = word_averaging_list(wv,X_temp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wqHx-w1XiXe-"},"source":["best_solution\n","X_new = np.multiply(X,best_solution)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VUT3-9e_d3EO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620656425330,"user_tz":-330,"elapsed":4500007,"user":{"displayName":"2K19/EP/036 HARSH MITTAL","photoUrl":"","userId":"14192984764423656819"}},"outputId":"c20979d3-4840-475c-d92d-b511b145ab19"},"source":["from sklearn.model_selection import train_test_split\n","train_data,test_data,train_labels,test_labels = train_test_split(X_new,y, test_size=0.05, random_state=42)\n","classifier = RandomForestClassifier(n_estimators = 100)\n","\n","# Scaling the X parameters i.e. X_train and X_test\n","from sklearn.preprocessing import MinMaxScaler\n","sc_X = MinMaxScaler()\n","train_data = sc_X.fit_transform(train_data)\n","test_data = sc_X.fit_transform(test_data)\n","\n","classifier.fit(train_data,train_labels)\n","Y_pred = classifier.predict(test_data)\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n","Y_test = test_labels\n","f1 = f1_score(Y_test, Y_pred)\n","ps = precision_score(Y_test, Y_pred)\n","rs = recall_score(Y_test, Y_pred)\n","asc = accuracy_score(Y_test, Y_pred)\n","print(f\"F1 Score {f1}\")\n","print(f\"Precesion Score {ps}\")\n","print(f\"Recall Score {rs}\")\n","print(f\"Accuracy Score {asc}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["F1 Score 0.6455485161720573\n","Precesion Score 0.8498683055311677\n","Recall Score 0.5204301075268817\n","Accuracy Score 0.7425526761927828\n"],"name":"stdout"}]}]}